{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.3 MiB for an array with shape (3, 579485) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(sequences, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Create sequences\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Split data into train-test\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, sequence_length)\u001b[0m\n\u001b[0;32m     28\u001b[0m sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m sequence_length):\n\u001b[1;32m---> 30\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbpm_scaled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhour_sin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhour_cos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m[i:i\u001b[38;5;241m+\u001b[39msequence_length]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m     sequences\u001b[38;5;241m.\u001b[39mappend(seq)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(sequences, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\chatbot-01\\python-backend\\venv\\lib\\site-packages\\pandas\\core\\frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12594\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\chatbot-01\\python-backend\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32me:\\chatbot-01\\python-backend\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.3 MiB for an array with shape (3, 579485) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess data with float32 type\n",
    "data = pd.read_csv('E:\\\\chatbot-01\\\\datasets\\\\hr\\\\heart_rate_data.csv')\n",
    "data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'])\n",
    "data = data.drop(['date', 'time'], axis=1)\n",
    "data = data.sort_values(by='datetime')\n",
    "\n",
    "# Scale BPM values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data['bpm_scaled'] = scaler.fit_transform(data[['bpm']])\n",
    "\n",
    "# Add time-based features\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24).astype('float32')\n",
    "data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24).astype('float32')\n",
    "\n",
    "# Define the sequence length and create sequences\n",
    "sequence_length = 5\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[['bpm_scaled', 'hour_sin', 'hour_cos']].values[i:i+sequence_length].astype('float32')\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences, dtype='float32')\n",
    "\n",
    "# Create sequences\n",
    "sequences = create_sequences(data, sequence_length)\n",
    "\n",
    "# Split data into train-test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(sequences, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Use smaller batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(input_dim, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(sequence_length, input_dim)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dense(sequence_length, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "generator = build_generator(input_dim=3, sequence_length=sequence_length)\n",
    "\n",
    "# Define the WGAN discriminator model\n",
    "def build_wgan_discriminator(sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(sequence_length, 3)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))  # No activation for WGAN\n",
    "    return model\n",
    "\n",
    "discriminator = build_wgan_discriminator(sequence_length)\n",
    "discriminator.compile(loss='mean_squared_error', optimizer=RMSprop(learning_rate=0.0001))\n",
    "\n",
    "# Build the WGAN\n",
    "def build_wgan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(sequence_length, 3))\n",
    "    generated_sequence = generator(gan_input)\n",
    "    gan_output = discriminator(generated_sequence)\n",
    "    gan = tf.keras.models.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='mean_squared_error', optimizer=RMSprop(learning_rate=0.0001))\n",
    "    return gan\n",
    "\n",
    "gan = build_wgan(generator, discriminator)\n",
    "\n",
    "# Use a generator for training\n",
    "def data_generator(data, batch_size, sequence_length):\n",
    "    while True:\n",
    "        idx = np.random.randint(0, len(data) - sequence_length, batch_size)\n",
    "        batch_sequences = np.array([data[i:i+sequence_length] for i in idx], dtype='float32')\n",
    "        yield batch_sequences, -np.ones((batch_size, 1))\n",
    "\n",
    "train_gen = data_generator(X_train, batch_size, sequence_length)\n",
    "\n",
    "# Train WGAN with data generator\n",
    "epochs = 1000\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    real_sequences = next(train_gen)[0]\n",
    "    noise = np.random.normal(0, 1, (batch_size, sequence_length, 3))\n",
    "    gen_sequences = generator.predict(noise)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real_sequences, -np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_sequences, np.ones((batch_size, 1)))\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, sequence_length, 3))\n",
    "    g_loss = gan.train_on_batch(noise, -np.ones((batch_size, 1)))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} - D loss: {0.5 * (d_loss_real + d_loss_fake)}, G loss: {g_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_kernel)",
   "language": "python",
   "name": "venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
